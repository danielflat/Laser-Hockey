{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create Gym environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "\n",
    "# Define self-play logic\n",
    "class SelfPlayEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(SelfPlayEnv, self).__init__()\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.action_space = self.env.action_space  # Agent 1 actions\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "        # Disturbance action space for Agent 2\n",
    "        self.disturbance_space = gym.spaces.Discrete(3)  # Left, None, Right\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset(seed = 0)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Agent 1 action\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "\n",
    "        # Agent 2 disturbance\n",
    "        disturbance = np.random.choice([-1, 0, 1])  # Apply random force\n",
    "        self.env.env.state[1] += disturbance * 0.01  # Modify velocity\n",
    "\n",
    "        # Adjust reward for self-play scenario\n",
    "        reward -= abs(disturbance * 0.1)  # Penalize disturbances\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def render(self, mode = \"human\"):\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "# Initialize self-play environment\n",
    "self_play_env = SelfPlayEnv()\n",
    "\n",
    "# Train PPO on the self-play environment\n",
    "model = PPO(\"MlpPolicy\", self_play_env, verbose = 1)\n",
    "model.learn(total_timesteps = 50000, progress_bar = True)\n",
    "\n",
    "# Evaluate and visualize the results\n",
    "obs, info = self_play_env.reset()\n",
    "rewards = []\n",
    "for _ in range(200):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, _ = self_play_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "        obs, info = self_play_env.reset()\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Rewards Over Time\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n"
   ],
   "id": "1ddc073b16900fd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "# Define Self-Play Environment\n",
    "class SelfPlayEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(SelfPlayEnv, self).__init__()\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.action_space = self.env.action_space  # Agent 1 actions\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "        # Disturbance action space for Agent 2\n",
    "        self.disturbance_space = gym.spaces.Discrete(3)  # Left, None, Right\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        obs, _ = self.env.reset(seed = seed, options = options)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Agent 1 action\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Agent 2 disturbance (random force)\n",
    "        disturbance = np.random.choice([-1, 0, 1])  # Apply random force\n",
    "        self.env.unwrapped.state[1] += disturbance * 0.01  # Modify velocity\n",
    "\n",
    "        # Adjust reward for self-play scenario\n",
    "        reward -= abs(disturbance * 0.1)  # Penalize disturbances\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "# Initialize Self-Play Environment\n",
    "self_play_env = SelfPlayEnv()\n",
    "\n",
    "# Train PPO on the Self-Play Environment\n",
    "model = PPO(\"MlpPolicy\", self_play_env, verbose = 1)\n",
    "model.learn(total_timesteps = 50000)\n",
    "\n",
    "# Evaluate and Visualize Results\n",
    "obs, _ = self_play_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "\n",
    "for step in range(200):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, _ = self_play_env.step(action)\n",
    "    rewards.append(reward)\n",
    "    steps.append(step)\n",
    "    if terminated or truncated:\n",
    "        obs, _ = self_play_env.reset()\n",
    "\n",
    "# Plotting the Rewards\n",
    "plt.plot(steps, rewards, label = \"Rewards\")\n",
    "plt.title(\"Rewards Over Steps in Self-Play Environment\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "803f8ea455abcc5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Dummy environment setup\n",
    "state_dim = 4\n",
    "num_actions = 3\n",
    "num_bins = 10\n",
    "\n",
    "# Discretize the value range\n",
    "min_value, max_value = -1, 1  # Example Q-value range\n",
    "bins = torch.linspace(min_value, max_value, num_bins)\n",
    "\n",
    "\n",
    "# Neural network for Q-value classification\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, num_actions, num_bins):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions * num_bins)\n",
    "        )\n",
    "        self.num_actions = num_actions\n",
    "        self.num_bins = num_bins\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1, self.num_actions, self.num_bins)\n",
    "\n",
    "\n",
    "# Initialize network, loss, and optimizer\n",
    "q_net = QNetwork(state_dim, num_actions, num_bins)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(q_net.parameters(), lr = 0.001)\n",
    "\n",
    "# Dummy training loop\n",
    "for epoch in range(100):\n",
    "    # Simulate a batch of states\n",
    "    states = torch.rand((32, state_dim))\n",
    "\n",
    "    # Simulate Q-values and actions\n",
    "    q_values = torch.rand((32, num_actions)) * (max_value - min_value) + min_value\n",
    "\n",
    "    # Assign each Q-value to a bin\n",
    "    target_bins = ((q_values - min_value) / (max_value - min_value) * (num_bins - 1)).long()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = q_net(states)  # Shape: (batch_size, num_actions, num_bins)\n",
    "\n",
    "    # Loss calculation\n",
    "    loss = 0\n",
    "    for action in range(num_actions):\n",
    "        loss += criterion(logits[:, action, :], target_bins[:, action])\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ],
   "id": "e5fc5c3f73a61a16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e20e4af0e59d60ae"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
