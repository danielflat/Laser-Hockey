{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Latent Dynamics Model Components\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes observations into a latent state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.fc(obs)\n",
    "\n",
    "\n",
    "class TransitionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts the next latent state given the current latent state and action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super(TransitionModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, action):\n",
    "        return self.fc(torch.cat([z, action], dim = -1))\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts the reward in the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "\n",
    "# Policy and Value Network\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    Outputs actions based on latent states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Tanh()  # Assuming actions are in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Outputs state values based on latent states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "\n",
    "# Model Predictive Control in Latent Space\n",
    "class MPC:\n",
    "    \"\"\"\n",
    "    Uses a learned model to plan optimal actions in latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, transition_model, reward_model, horizon, num_samples, action_dim):\n",
    "        self.encoder = encoder\n",
    "        self.transition_model = transition_model\n",
    "        self.reward_model = reward_model\n",
    "        self.horizon = horizon\n",
    "        self.num_samples = num_samples\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def plan(self, obs):\n",
    "        \"\"\"\n",
    "        Plan optimal actions by sampling and evaluating trajectories.\n",
    "        \"\"\"\n",
    "        z = self.encoder(obs)  # Encode observation to latent state\n",
    "        best_reward = -float('inf')\n",
    "        best_action_seq = None\n",
    "\n",
    "        # Sample action sequences\n",
    "        action_sequences = torch.randn((self.num_samples, self.horizon, self.action_dim))\n",
    "\n",
    "        for actions in action_sequences:\n",
    "            total_reward = 0\n",
    "            z_temp = z\n",
    "            for a in actions:\n",
    "                z_temp = self.transition_model(z_temp, a.unsqueeze(0))\n",
    "                total_reward += self.reward_model(z_temp)\n",
    "            if total_reward > best_reward:\n",
    "                best_reward = total_reward\n",
    "                best_action_seq = actions\n",
    "\n",
    "        return best_action_seq[0]  # Return the first action in the sequence\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "def train():\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    latent_dim = 16\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(obs_dim, latent_dim)\n",
    "    transition_model = TransitionModel(latent_dim, action_dim)\n",
    "    reward_model = RewardModel(latent_dim)\n",
    "    policy = Policy(latent_dim, action_dim)\n",
    "    value_network = ValueNetwork(latent_dim)\n",
    "\n",
    "    # Optimizers\n",
    "    encoder_optim = optim.Adam(encoder.parameters(), lr = 1e-3)\n",
    "    transition_optim = optim.Adam(transition_model.parameters(), lr = 1e-3)\n",
    "    reward_optim = optim.Adam(reward_model.parameters(), lr = 1e-3)\n",
    "    policy_optim = optim.Adam(policy.parameters(), lr = 1e-3)\n",
    "    value_optim = optim.Adam(value_network.parameters(), lr = 1e-3)\n",
    "\n",
    "    mpc = MPC(encoder, transition_model, reward_model, horizon = 10, num_samples = 100, action_dim = action_dim)\n",
    "\n",
    "    for episode in range(1000):  # Number of episodes\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype = torch.float32).unsqueeze(0)\n",
    "            action = mpc.plan(obs_tensor)\n",
    "            action = action.detach().numpy()\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Encode states and train latent dynamics\n",
    "            z = encoder(obs_tensor)\n",
    "            z_next = encoder(torch.tensor(next_obs, dtype = torch.float32).unsqueeze(0))\n",
    "            predicted_z_next = transition_model(z, torch.tensor(action, dtype = torch.float32).unsqueeze(0))\n",
    "            reward_pred = reward_model(z)\n",
    "\n",
    "            # Compute losses\n",
    "            reconstruction_loss = nn.MSELoss()(predicted_z_next, z_next)\n",
    "            reward_loss = nn.MSELoss()(reward_pred, torch.tensor([reward], dtype = torch.float32))\n",
    "            value_loss = nn.MSELoss()(value_network(z), torch.tensor([reward], dtype = torch.float32))\n",
    "\n",
    "            # Update models\n",
    "            encoder_optim.zero_grad()\n",
    "            transition_optim.zero_grad()\n",
    "            reward_optim.zero_grad()\n",
    "            policy_optim.zero_grad()\n",
    "            value_optim.zero_grad()\n",
    "\n",
    "            (reconstruction_loss + reward_loss + value_loss).backward()\n",
    "\n",
    "            encoder_optim.step()\n",
    "            transition_optim.step()\n",
    "            reward_optim.step()\n",
    "            value_optim.step()\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "\n",
    "train()\n"
   ],
   "id": "d460bd78e673dab1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9b6eaa5b84e8278b"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
